#import "../../tem/beamer.typ": beamer
#show: beamer.with(
  title: "基于 SAM 大模型的肝脏肿瘤分割软件开发",
  subtitle: "5-23 答辩",
  author: "答辩人：王恺                     指导老师：刘琨",
  date: "质量技术监督学院   2020级测控普通班  2024-05-22",
)

= 绪论
<绪论>
== 研究的背景及意义
<研究的背景及意义>
//肝脏肿瘤是全球范围内导致死亡的主要原因之一，计算机断层扫描（CT）是检测和评估肝脏肿瘤的重要影像学方法。为了提高诊断效率和准确性，自动化的图像分割技术应运而生。近年来，深度学习技术在医学图像分割领域取得了显著进展。然而，深度学习方法依赖于大量标注数据进行训练，而医学影像数据的标注成本高昂，需要专业的医学人员参与。

近年来，深度学习在图像处理和计算机视觉领域取得了显著进展，其在医学图像分割任务中展现出了巨大潜力。众多深度学习模型，在该领域取得了优异的结果。这些研究不仅提高了分割的准确性，而且大大缩短了处理时间。

Meta 提出的 Segment
Anything
Model（SAM），作为一种基于深度学习的全新模型架构，已在多个自然图像处理基准上显示出优异的性能。SAM 的引入为医学图像，特别是复杂的肝脏肿瘤分割任务，提供了新的可能性。


SAM 模型是 Meta AI 提出的一个基于 Transformer
的深度学习模型，它是在超过一亿张图像和相应标记的基础之上训练得来，用于执行零样本学习（Zero-shot
Learning）中的图像分割任务，它能够通过接收简单的标注提示（如点、框等），自动产生精确的图像分割，这在处理医学图像中的不同器官和病变（如肝脏肿瘤）时，表现出独特的优势。使用
SAM 模型应用与医学图像处理具有以下优势：

- 泛化能力强：SAM通过在大量图像上进行预训练，学习了丰富的视觉特征表示，具备强大的泛化能力，能够应对多样的图像分割任务。
- 少样本学习：在具有少量标注数据的情况下，SAM
  仍然能表现出良好的分割效果，这对于医学影像分析尤其重要，因为高质量的医学标注数据往往难以获得。
- 自适应性强：作为一种自适应的模型，SAM
  能够根据不同的输入图像和分割任务调整其行为，这使得它在面对肝脏肿瘤这种形态多样、边界模糊的目标时，表现出比传统方法更好的分割精度。

// 在医学图像分析领域，特别是肝脏肿瘤的精确分割中，选择合适的图像分割算法对于提高诊断准确性和疾病管理至关重要。基于SAM大模型的肝脏肿瘤分割软件开发具有重要的临床意义和广阔的应用前景。通过利用SAM模型的高泛化能力和出色的图像处理性能，可以极大地提升肝脏肿瘤的诊断精度，助力医生更有效地制定治疗计划，最终提高患者的生存率。开发此类高效的肝脏肿瘤分割软件不仅能推动医学影像分析技术的进步，也将为全球癌症治疗带来积极的影响。


== 国内外研究现状
- 基于传统方法的分割

  - 阈值法
  - 主动轮廓法
  - 区域生长法。

传统的分割方法噪声敏感，特征选择和提取过程复杂且依赖经验，难以确保正确分割。

- 基于深度学习的分割
  - U-Net 及其变种
  - 基于卷积对抗神经网络（CNN）的分割方法
  - 基于 Transformer 的方法（如SAM）
基于传统深度学习模型的分割方法对数据量需求大，所需数据的标注成本高，尤其是肿瘤分割等任务。泛化能力弱，模型在一个数据集上表现良好，但在其他数据集上性能可能显著下降。


//**CNN**是深度学习技术中最早应用于医学图像分割的算法之一，它通过层叠的卷积层来提取图像特征，已成功应用于多种医学图像处理任务。然而，尽管CNN在特征提取方面表现出色，但其对训练数据的依赖性较大，需要大量精确标注的数据，这在医学领域往往是一个限制因素。此外，CNN模型的泛化能力受限于训练数据的多样性和质量。对数据的训练数据的大量需求增加了其训练开发难度，为该方面的实际应用增加了成本。

//**U-Net**是为医学图像分割特别设计的网络结构，它通过特有的跳跃连接和上采样策略，有效地保留了图像的细节信息，适合于处理样本量较少的医学图像数据。U-Net 及其变种神经网络模型在肝脏肿瘤等复杂结构的分割任务中表现出良好的性能。然而，U-Net 对图像中的噪声和伪影比较敏感，这可能影响其在实际临床应用中的分割准确性。

//近期，基于 Transformer 的 Segment Anything Model（SAM）展示了其在图像分割任务中的卓越潜力。不同于传统的 CNN 和 U-Net，SAM 采用 Transformer 架构处理图像中的长距离依赖，能够捕获更加丰富的上下文信息。SAM的一个显著优势是其零样本学习能力，即在未见过的新图像上，仅通过少量的提示（如点或框）就能实现准确的分割。这一优势特性在处理多变的医学图像，尤其是肝脏肿瘤图像时显得尤为重要，因为这些图像常常包含不规则的肿瘤边界和复杂的背景结构。

//总体来说，虽然多种方法各有优势，SAM 方法由于其强大的零样本学习能力和优异的上下文捕获能力，展现了在医学图像，特别是在肝脏肿瘤分割方面的独特优势。

//////由于过去分割方法的种种缺陷，我们使用了 SAM 大模型进行肝脏肿瘤分割任务的开发，这也是本研究主要的创新点。

== 研究内创新点
<研究内容及创新点>

本文的创新之处在于使用了大型自监督学习模型 SAM
开发肝脏肿瘤分割软件，该研究方法的优势体现在以下几个方面：

+ 提高肝脏肿瘤分割的准确性和灵活性：利用SAM模型的强大特征提取能力，可以更准确地识别和分割肝脏中的肿瘤组织，特别是在肿瘤边界模糊或与周围组织对比度低的情况下。

+ 减少对标注数据的依赖：由于SAM模型基于自监督学习，在训练过程中不完全依赖标注数据，通过利用自监督学习，该软件能够利用未标注的医学图像进一步提升模型的性能和泛化能力。

+ 提升分割速度，支持实时应用：针对 SAM
  模型的实现进行了微调优化，使得肝脏肿瘤分割软件能够快速处理图像，满足临床环境中对实时或近实时分析的需求。

= 理论技术基础
<技术原理学习>
== 肝脏肿瘤 CT 图像
<肝脏肿瘤-ct-图像>
计算机断层扫描（Computed Tomography，CT）技术是一种先进的医学成像技术，通过利用X射线和计算机技术生成身体内部详细的横断面图像。//CT扫描利用旋转的X射线管和对面的探测器阵列从不同角度获取人体的X射线数据。这些数据被计算机处理，重建出二维切片图像。这些切片图像可以通过计算机系统堆叠组合成三维图像，从而提供详细的内部结构视图以供放射科医生查看图像，以识别疾病或异常。生成的报告发送给主治医生，以便进行进一步治疗决策。

#figure(image("腹部.png", width: 13cm),
  caption: [
    腹部 CT 图像，左上三图为腹部CT图像 MPR 预览，右下为 3D 合成图像预览
  ]
)

CT 图像的优点有以下几点：

+ #strong[高分辨率];：能够清晰显示细微结构和病变。
+ #strong[快速成像];：现代CT扫描仪能够在几秒钟内完成扫描，适合紧急情况。
+ #strong[多平面成像];：能够生成横断面、冠状面和矢状面图像，提供多角度视图。
+ #strong[三维重建];：通过多个切片图像的组合，生成精确的三维图像。

//////CT扫描技术在头部和脑部成像、胸部成像、腹部和盆腔成像、骨骼和关节成像、血管成像等医学领域具有广泛应用，用于评估各类肿瘤等多种身体疾病异常如肝脏肿瘤的治疗。

//肝脏肿瘤的分割是医学图像处理中一项具有挑战的任务，CT 图像分割作为一种非侵入性的诊断方法，可以帮助医生更准确地确定肝脏肿瘤的位置、形状和大小，为后续治疗提供重要的参考依据，

但肝脏肿瘤的分割面临以下难点:

+ #strong[肿瘤形态多样性]
  //肝脏肿瘤的形态、大小和位置具有高度多样性，导致图像中肿瘤区域的外观各异。肿瘤可能是单个的，也可能是多发的，其形状可以是规则的，也可以是非常不规则的。这种多样性增加了分割的难度。
+ #strong[图像噪声和伪影] CT
  //图像中常常存在噪声和伪影，这些干扰信息会影响图像的清晰度和对比度，进而影响肿瘤区域的准确识别和分割。特别是在低剂量 CT 扫描中，噪声问题更加突出。
+ #strong[肝脏结构复杂]
  //肝脏内部结构复杂，包含血管、胆管等多种解剖结构，这些结构在 CT 图像中可能与肿瘤区域混淆，增加了分割的难度。此外，肝脏边界与周围器官（如胃、肠、肾）接近，界限模糊，进一步增加了分割任务的复杂性。
+ #strong[肿瘤与正常组织对比度低]
  //肿瘤与正常肝脏组织的密度对比度往往较低，特别是一些小肝癌或早期肝癌，难以在 CT 图像中明显区分。这种低对比度使得自动化分割算法难以准确区分肿瘤和正常组织。
+ #strong[患者间差异]
  //不同患者的肝脏形态和肿瘤特征存在显著差异，这种个体差异增加了图像分割算法的泛化难度。一种分割方法可能在某些患者图像上表现良好，但在另一些患者图像上效果不佳。
+ #strong[数据标注困难]
  //高质量的肝脏肿瘤分割需要大量准确的标注数据，这些标注通常由放射科医生手动完成，耗时费力且容易受主观影响。这限制了训练和验证分割算法所需的数据量和质量。

//如上所述，对 CT 图像中的肝脏肿瘤进行分割困难极大，人类极难分辨如图 2.2的微小肿瘤图像，这些肿瘤几乎肉眼不可见。因此，使用 SAM大模型的肝脏肿瘤分割方法具有重要意义。

//#align(right)[
//  #figure(
//    image("肝脏肿瘤示意.png", width: 10cm),
//    caption: [
//      肝脏肿瘤 CT 图像示意，从左到右依次为原 CT
//      图像，肝脏标注，肿瘤标注。
//    ]
//  )
//]
== 深度学习基本原理
<深度学习基本原理>
深度学习是机器学习的一个分支，基于多层神经网络模拟人脑处理数据的方式。它涵盖从输入层接收数据，通过多个隐藏层处理，到输出层产生结果的过程。//每个神经元根据权重和偏差处理输入，并通过激活函数引入非线性，使得模型能处理复杂任务。训练神经网络涉及使用损失函数评估预测误差，并通过反向传播和梯度下降等优化算法调整网络参数，以减少误差。此外，为避免过拟合，常用正则化技术如舍弃法（Dropout）。深度学习在图像识别、语音处理等领域展示出显著的应用潜力，通过适当的网络设计和算法优化，可以有效从大量数据中学习复杂的规律。深度学习模型能够从大量数据中自动学习到复杂的特征表示，这一点在图像识别、语音识别等任务中尤为显著。

=== Pytorch框架
<pytorch框架>
PyTorch
是一个开源的机器学习库，广泛用于计算机视觉和自然语言处理等领域。//它由Facebook的人工智能研究团队开发，并得到了包括微软、Salesforce等多家大公司的支持和贡献。PyTorch是一种强大的深度学习框架，适合从学术研究到商业应用的广泛用途。它的灵活性、用户友好的设计以及强大的社区支持使其成为当前最受欢迎的深度学习框架之一。

//- PyTorch 以其动态计算图（Dynamic Computation
//  Graphs），即“define-by-run”方法论而闻名。这种方式让每一次的网络操作都可以动态//地改变计算图，提供了极高的灵活性和直观操作方式，使得模型设计和调试更为简单直//接。
//- PyTorch
//  提供了丰富的API，这些API设计直观并易于理解，极大地简化了深度学习模型的开发过//程。它支持大量的预定义层，如全连接层、卷积层、池化层等，以及多种损失函数和优//化器，这使得构建复杂的神经网络变得更加容易。
//- PyTorch
//  拥有一个活跃的社区，提供大量的教程、工具和预训练模型，这些资源可以帮助用户快//速开始项目并解决遇到的问题。此外，PyTorch
//  与许多研究项目和商业应用相结合，形成了一个强大的生态系统。
//- 由于其灵活性和简便性，PyTorch
//  在学术界特别受欢迎，成为许多最新研究论文的首选框架。它支持快速实验的特点，使//研究人员能够验证新想法并迅速实现原型。
//- PyTorch 不仅易于使用，而且在性能方面也非常优秀。它可以无缝地运行在 CPU
//  和GPU 上，通过优化的C++库支持高效的内存使用和计算速度。
//- PyTorch
//  提供了与其他重要科学计算库的接口，如NumPy和SciPy，以及可视化工具如//TensorBoard。此外，它还支持ONNX（Open
//  Neural Network
//  Exchange）格式，这使得在不同框架之间转换模型变得更加容易。

本文研究将使用 PyTorch 框架微调 SMA 模型。//在Python环境下，利用Pytorch库构建SAM大模型的基础框架。

=== Transformer
<transformer>
Transformer是一种基于自注意力机制的神经网络架构，最初由Vaswani等人在2017年的论文《Attention
Is All You
Need》中提出。Transformer模型在自然语言处理（NLP）任务中取得了巨大成功，并被广泛应用于其他领域如计算机视觉（例如VIT，
vision Transformer）。

==== Transformer模型的组成
<transformer模型的组成>
#figure(image("./Transformer结构图.png",width: 10cm),
  caption: [
    图 2.3 Transformer 结构图
  ]
)

//如图，Transformer模型主要由编码器（Encoder）和解码器（Decoder）两部分组成，每部分由多个相同的层（layers）堆叠而成。每个层主要包含两个子层（sublayers）：

//==== 1. 编码器（Encoder）
//<编码器encoder>
//每个编码器层包含以下两个子层： - 自注意力层（Self-Attention Layer）];：计算输入序列中每个位置的注意力分数，捕捉输入序列中的全局依赖关系。\- #strong[前馈神经网络（Feed-Forward Neural Network，FFN）];：对每个位置的输入进行独立的非线性变换。

//每个编码器层的输出会通过残差连接（residual connection）和层归一化（layer normalization）后，传递给下一层。

//==== 2. 解码器（Decoder）
//<解码器decoder>
//每个解码器层包含三个子层： - #strong[自注意力层（Self-Attention Layer）];：与编码器相同，但只对解码器输入的前缀部分计算注意力分数。 -#strong[编码器-解码器注意力层（Encoder-Decoder Attention Layer）];：将解码器的输入与编码器的输出结合，计算注意力分数，捕捉输入与输出之间的依赖关系。 \- #strong[前馈神经网络（Feed-Forward Neural Network， FFN）]：与编码器相同，对每个位置的输入进行独立的非线性变换。

//同样，每个解码器层的输出也会通过残差连接和层归一化后，传递给下一层。

== （Vision Transformer，VIT）
//<vision-transformervit>
VIT 模型是 Transformer 模型在 CV 领域的应用，用于把图像映射到特征空间，具体结构如图所示。//将图片输入图像编码器后，首先使用卷积分块缩小尺寸，之后借助 Flatten 函数将分块的图像转换成向量，并与位置信息（position embedding）相加，相加结果经过 Transformer Encoder 处理生成特征图，最后再通过两层卷积对特征图进行降维，得到图像嵌入（Image Bedding）。位置信息是初始为 0 的参数矩阵，用于后续位置更新。SAM模型图像编码器使用了掩码自编码器（masked autoencoders，MAE）方法预训练的 VIT（Vision Transformer）模型。MAE 是一种自监督学习方法，能够将输入的图片分块，并随机进行遮盖，之后借助编码器和解码器重构这些被遮盖的像素。

#figure(image("VIT结构图.png",width: 12cm),
  caption: [
    VIT 结构图
  ]
)

//=== 自注意力机制（Self-Attention Mechanism）
//<自注意力机制self-attention-mechanism>
//自注意力机制是Transformer的核心组件，它允许模型在计算每个位置的表示时，考虑整个序列中的其他位置。其工作原理如下：

//+ #strong[输入变换];：
//  - 将输入序列（例如词嵌入）变换为查询（query）、键（key）和值（value）向量：
//    $ Q = X W_Q ， quad K = X W_K ， quad V = X W_V $ 其中，$W_Q$，
//    $W_K$， $W_V$ 是学习到的权重矩阵。
//+ #strong[计算注意力分数];：
//  - 计算查询与键的点积，并进行缩放和软最大化（softmax），得到注意力分数：
//    $ upright("Attention") (Q ， K ， V) = upright("softmax") (frac(Q K^T, sqrt(d_k))) V $
//    其中，$d_k$ 是键向量的维度。
//+ #strong[加权求和];：
//  - 使用注意力分数对值向量进行加权求和，得到每个位置的新表示。

//=== 多头注意力机制（Multi-Head Attention）
//<多头注意力机制multi-head-attention>
//为了捕捉不同子空间的信息，Transformer使用了多头注意力机制。具体做法是将查询、键、值向量分成多个头，每个头独立计算注意力，然后将结果拼接在一起，并通过线性变换得到最终的输出：
//$ upright("MultiHead") （ Q ， K ， V ） = upright("Concat") （ upright("head")_1 ， dots.h ， upright("head")_h ） W_O $
//其中，$upright("head")_i = upright("Attention") （ Q W_(Q_i) ， K W_(K_i) ， V W_(V_i) ）$。

//=== 位置编码（Positional Encoding）
//<位置编码positional-encoding>
//由于Transformer没有卷积或递归结构，它无法直接捕捉输入序列的位置信息。为此，Transformer在输入中添加了位置编码，提供位置信息。位置编码可以是固定的，也可以是可学习的，其形式通常为：
//$ upright("PE")_((p o s ， 2 i)) = sin (\) frac(p o s, 10000^(2 i \/ d))) quad upright("PE")_((p o s ， 2 i + 1)) = cos (\) frac(p o s, 10000^(2 i \/ d))) $
//其中，$p o s$ 是位置，$i$ 是维度索引。

//=== 总结
//<总结>
//Transformer通过自注意力机制和前馈神经网络，在自然语言处理和其他领域中实现了卓越的性能。其核心优势在于能够捕捉全局依赖关系，易于并行计算，并且具有良好的扩展性。自推出以来，Transformer模型已经衍生出许多变种和改进版本，并在许多任务中取得了显著成果。

== 分割大模型（SAM大模型）
<分割大模型sam大模型>
//本文主要研究通过对 SAM 大模型的微调训练，以适应在医学图像处理，尤其是对腹部 CT 扫描图像的肝脏肿瘤分割。

//=== 背景
<背景>
SAM（Segment Anything Model）是近年来出现的一种新型深度学习模型，由 Meta
的 FAIR 实验室提出，旨在通过单一的模型实现对任何对象的高精度分割//。SAM大模型通过大规模数据的预训练和针对特定任务的微调，实现了对不同对象类型的准确识别和分割。

//SAM模型的强大之处在于它借鉴了自然语言处理领域的 Foundation Model。Foundation Model 在预训练阶段学习了大量的语言知识，从而能够在各种语言任务中表现出色。同样地，SAM模型在预训练阶段学习了大量的视觉知识，使其能够适应各种下游图像分割任务。

//////SAM 模型的核心思想是使用提示学习来适应不同的分割问题。提示学习是一种通过给模型提供一些指导信息来帮助其完成任务的方法。在SAM模型中，设计了一种可提示的分割任务，使模型可以根据不同的任务需求进行微调。这种可提示的特性使得SAM模型能够轻松地适应各种复杂的分割问题，例如语义分割、实例分割等。

//为了实现强大的零样本学习能力，SAM模型在预训练阶段使用了大规模的数据集进行训练。通过在大量图像数据上的学习，模型能够提取出通用的视觉特征，从而在面对新的、未见过的图像时，能够快速地进行有效地分割。这种零样本学习能力使得SAM模型在处理新场景、新任务时具有很大的优势，例如将其微调，用于肝脏肿瘤分割软件的开发。

//在网络数据集上预训练的大语言模型具有强大的 zero-shot（零样本）和 few-shot（少样本）的泛化能力，这些“基础模型”可以推广到超出训练过程中的任务和数据分布，这种能力通过“prompt engineering”实现，具体就是输入提示语得到有效的文本输出，使用网络上的大量文本资料库进行缩放和训练后，可以看出这种零样本和少样本的训练的模型比专一性质功能模型效果还要好。数据集越大，效果越明显。

//视觉任务方面也对这种基础模型进行了探索，比如CLIP和ALIGN利用对比学习，将文本和图像编码进行了对齐，通过提示语生成 image encoder，就可以扩展到下游任务，比如生成图像。

SAM通常在自然图像上表现优异，但是在特定领域如医疗影响，遥感图像等，由于训练数据集缺乏这些数据，SAM
的效果并不是理想。因此，在特定数据集上微调 SAM 是十分有必要的。

//=== SAM 模型结构
<sam-模型结构>
//如图所示SAM模型结构主要包括以下几个部分：

//+ 图像编码器（Image Encoder）：

//- 接收输入图像并提取图像特征。
//- 通常使用预训练的视觉模型，如 VIT（vision Transformer）。

//#block[
//#set enum(numbering: "1.", start: 2)
//+ 提示编码器（Prompt Encoder）：
//]
//
//- 处理用户提供的提示信息（如点、框、文本等）。
//- 提取提示信息的特征表示。
//
//#block[
//#set enum(numbering: "1.", start: 3)
//+ 掩码解码器（Mask Decoder）：
//]
//
//- 将图像特征和提示特征进行融合。
//- 生成分割掩码。
//
//#block[
//#set enum(numbering: "1.", start: 4)
//+ 特征融合与加权（Feature Fusion and Weighting）：
//]
//
//- 通过逐元素相乘（element-wise multiplication）对图像特征进行加权。
//- 利用提示特征对图像特征进行调整，提升分割精度。
//
//#block[
//#set enum(numbering: "1.", start: 5)
//+ 最终输出：
//]
//
//- 生成图像的分割掩码。

#figure(image("SAM结构图.png"),
  caption: [
    图 2.5 SAM 模型的结构
  ]
)

==== SAM 模型运行步骤
<sam-模型运行步骤>
- 输入图像（Image）：模型接收一幅输入图像。
- 图像编码器（Image
  Encoder）：输入图像经过图像编码器（通常是VIT），提取出高维度的图像特征。
- 提示编码器（Prompt
  Encoder）：用户提供的提示信息（如标记点、框等）经过提示编码器，提取出提示特征。
- 特征融合与加权（Feature Fusion and
  Weighting）：图像特征和提示特征进行融合。具体操作是通过逐元素相乘（element-wise
  multiplication），使提示特征对图像特征进行加权。
- 掩码解码器（Mask
  Decoder）：融合后的特征通过掩码解码器，生成图像的分割掩码。
- 最终输出（Final Output）：输出最终的分割掩码，表示图像中的分割区域。

//==== 图像编码器（Image Encoder）
<图像编码器image-encoder>
//利用 meta 预训练的 VIT，最低限度适应高分辨率的输入，该编码器在提示编码器（prompt encoder）之前，对每张图像只运行一次。

//输入（c，h，w）的图像，对图像进行缩放，按照长边缩放成1024，短边不够就pad，得到（c，1024，1024）的图像，经过图像编码器（image encoder）处理，得到对图像16倍下采样的特征（feature），大小为（256，64，64）。

//==== 提示编码器（Prompt Encoder）
<提示编码器prompt-encoder>
//提示编码器分成2类：稀疏的（点，box，文本），稠密的（mask）。

//- 点（point）:映射到256维的向量，包含代表点位置的位置编码（positional
  encoding），加 2 个代表该点是前景/背景的可学习的嵌入（embedding）。
//- 框（box）:用一个嵌入对表示（1）可学习的嵌入代表左上角（2）可学习的嵌入代表右下角
//- 文本：通过CLIP模型进行文本编码。
//- 掩码（mask）:说用掩码的特征对图像的特征进行加权使特定区域被放大或抑制。

//用输入图像 $1 / 4$ 分辨率的掩码，然后用 （2，2）卷积核步长为 2，输出通道数为4和16，再用 （1，1）卷积核将通道数量升到256。掩码和图像嵌入（iamge embedding）通过逐元素相乘（element-wise），也就是
//== 本章小结
//<本章小结-1>
//本章探讨讲述了计算机断层图像扫描扫描（Computed Tomography，CT）、深度学习的基本原理、PyTorch 框架的特点，以及 Transformer 和 SAM（Segment Anything Model）模型的结构与机制。深度学习通过多层神经网络模拟人脑处理数据的方式，从输入层接收数据，经隐藏层处理后，在输出层产生结果。其关键在于使用损失函数评估预测误差，并通过反向传播和梯度下降等优化算法调整网络参数，以减少误差。PyTorch 是一个开源的机器学习库，以动态计算图和易用的 API 著称，广泛应用于计算机视觉和自然语言处理领域。Transformer 是一种基于自注意力机制的神经网络架构，通过编码器和解码器捕捉输入序列的全局依赖关系，易于并行计算且具有良好的扩展性。SAM 模型由 Meta 的 FAIR 实验室提出，旨在通过单一模型实现高精度分割。它通过大规模数据预训练和提示学习，适应各种下游图像分割任务。其结构包括图像编码器、提示编码器、特征融合与加权、掩码解码器和最终输出部分，通过对图像特征和提示特征的融合与加权，生成高精度分割掩码。这些技术和模型展示了深度学习在图像处理领域的强大性能和应用前景，为后续研究奠定了基础。

= 实验研究
<实验研究>
== 数据集处理
<准备数据集>
数据集共包括 20 个病例的 CT 图像，每个图像都附有肝脏肿瘤的标注（如表1），便于对数据集的训练与测试。//对数据集进行随机分组，80% 作为训练集，10% 作为验证集，10% 作为测试集。在腹部 CT 图像中，肝脏和相邻的组织器官呈现紧贴的状态，并且它们之间的边界不清晰以及对比度较低，所以 CT 图像在输入到模型之前需要对其进行一些处理，减少无关噪声的干扰，增强图像的对比度。

//除上述处理操作外，数据集还进行了旋转、平移以及缩放，以此提升模型的泛化能力和鲁棒性，避免过拟合情况的发生。

#figure(
  align(center)[#table(
    columns: (24.14%, 70.69%, 5.17%),
    align: (auto,auto,auto,),
    table.header([文件名], [文件内容], [],),
    table.hline(),
    [PATIENT\_DICOM], [DICOM 格式的匿名患者图像], [],
    [LABELLED\_DICOM], [DICOM
    格式分割的各个感兴趣区域对应的标签图像], [],
    [MASKS\_DICOM], [包含每个 mask 的 DICOM
    图像的各个感兴趣区域的名称对应的一组新子文件夹], [],
  )],
  caption: [
    数据集内容
  ]
  , kind: table
  
  )
读取数据集并进行数据分割和图像预处理，CT值转换、窗口化、直方图均衡化、归一化、定义增强参数。模型训练和工作过程中的数据存储。

//读取数据：从原始的 DICOM
//格式文件中读取数据，并转换成数组格式以便于处理。
//
//数据预处理
//
//- CT值转换：将CT图像的原始值转换为 Hounsfield Units
//  （HU），以标准化不同设备和协议下的图像数据。
//- 窗口化：应用窗口化技术以增强图像中特定组织或结构的可视化。
//- 直方图均衡化：使用CLAHE方法改善图像的对比度，特别是对于医学图像中经常存在的低对比度区域。
//- 归一化];：将图像数据缩放到$[0,1]$区间，以便于网络处理。
//- 目标区域提取：只保留包含肝脏的腹部切片，排除其他不相关的图像。
//
//数据增强
//
//- 定义增强参数：设置旋转、平移、剪切和缩放等增强操作的参数。
//- 应用数据增强：对图像和掩码（mask）应用相同的变换，以增加数据集的多样性并提高模型的泛化能力。
//
//数据存储
//
//- HDF5格式：将处理后的数据存储为 HDF5 格式，以减少 I/O 操作并方便数据共享。
//- 自定义类：使用 HDF5DatasetWriter 和 HDF5DatasetGenerator 类来管理数据的读写操作。
//////使用HDF5格式存储图像，pythong库处理。
== 模型架构调整
//<模型架构调整>
加载 meta 预训练的 SAM 模型作为基础模型。//加载预训练模型的权重，并设置为微调模式。为了保留预训练模型中已经学习到的有用特征，通常会冻结模型的输入层，只微调模型的顶部层。这可以通过设置模型中某些层的参数不可训练为“True”来实现。冻结输入层可以防止在训练过程中更新这些层的权重，从而保留原始模型的特征表示。只微调顶部层有助于模型适应新任务的特定需求，同时减少训练时间和计算资源。

自定义Prompt Encoder：为了更好地适应肝脏CT图像特征，设计或调整Prompt
Encoder部分，增加对解剖结构敏感的特征提取层。

微调 Mask Decoder：在 Mask Decoder 部分，保持原有Transformer架构的基础上，微调最后几层的权重，使其对肝脏和肿瘤边缘更加敏感。

多尺度特征融合：引入多尺度特征融合策略，如U-Net结构中的跳跃连接，结合不同层级的特征来提高分割精度。

//多尺度特征融合策略（Multi-Scale Feature Fusion）是一种在深度学习模型中处理和整合不同尺度的特征，以增强模型对多样性和复杂性数据的适应能力的技术。特别是在图像分割任务中，多尺度特征融合可以帮助模型更好地捕捉细节和全局信息，从而提高分割精度和鲁棒性。
//为什么需要多尺度特征融合？

//在图像分割任务中，不同尺度的特征包含不同层次的信息：

//    低层次特征：通常包含边缘、纹理等局部信息，有助于捕捉细节。
//    高层次特征：包含语义信息，有助于理解图像的整体结构和上下文关系。

//通过融合不同尺度的特征，可以让模型同时利用这些信息，达到更好的分割效果。多尺度特征融合策略的具体实现

//特征金字塔网络（Feature Pyramid Network, FPN）：
    //FPN 是一种经典的多尺度特征融合方法，通常用于目标检测和图像分割任务。它通过自底向上的特征提取和自顶向下的特征融合，构建了一系列不同尺度的特征图。
    //在 FPN 中，每一层的特征图不仅包含该层的特征，还融合了来自更高层次（更细尺度）特征图的信息。


== 训练策略
<训练策略>
迁移学习:
从预训练的SAM模型开始，冻结部分或全部编码器层，仅微调解码器层和可能新增的自定义层。

学习率调度：使用余弦退火学习率调度（Cosine
Annealing），初始学习率设为$1^(- 4)$，随着训练进行逐渐降低。

//- 学习率在训练过程中按照余弦函数变化，使得学习率在训练初期较高，逐渐减小到最小值，然后再次升高。
- 公式：$ upright("lr")_t = upright("lr")_min + 1 / 2 （ upright("lr")_0 - upright("lr")_min ） （ 1 + cos （ T_(upright("cur")) / T_(upright("max")) pi ） ） $
  //其中，$t e x t l r_t$为第$t$个训练周期（epoch）的学习率，$upright("lr")_0$为初始学习率，$upright("lr")_min$为最小学习率，$T_(upright("cur"))$为当前训练周期（epoch），$T_(upright("max"))$为总的训练周期（epoch）数。

//混合精度训练: 利用混合精度训练加速训练过程并节省 GPU 内存，例如使用 NVIDIA 的 Apex 库。

//数据增强: 实施旋转、翻转、缩放、剪切、亮度变化等数据增强操作，并利用 Rand Augment 等自动化增强策略增加多样性。

使用训练数据集进行迭代训练。在训练过程中，需要监控模型的性能指标如训练损失和验证损失。为了确保模型的泛化能力，可以在每个训练周期结束后使用验证集进行评估。通过细致地微调，提升模型对肝脏肿瘤图像特点的学习能力和分割准确性。


=== 损失函数设计
<损失函数设计>
Dice Loss + BCE Loss: 使用结合了Dice
Loss和二元交叉熵损失（BCE）的复合损失函数，Dice
Loss关注区域的重叠程度，BCE则强调分类的准确性。

加权损失:
对肿瘤区域施加更高的权重，因为肿瘤区域相对于正常肝脏组织更为关键，且数量较少，这样可以平衡类别不均衡问题。

==== Dice 系数
<dice-系数>
#strong[定义];：Dice 系数（也称为 Dice
相似系数）是一种用于衡量两个样本集合相似度的统计指标，尤其适用于评估图像分割的精度。

$ upright("Dice") = frac(2 T P, 2 T P + F P + F N) = frac(2 lr(|A sect B|), lr(|A|) + lr(|B|)) $

//- $T P$ （True Positive）：真正例数，即正确预测为正类的像素数。
//- $T N$（True Negative）：真负例数，即正确预测为负类的像素数。
//- $F P$（False Positive）：假正例数，即错误预测为正类的像素数。
//- $F N$（False Negative）：假负例数，即错误预测为负类的像素数。
//- $A$ 为预测的二值掩码。
//- $B$ 为真实的二值掩码。
//- $lr(|A sect B|)$ 为预测与真实掩码的交集像素数。
//
//#strong[优点];：Dice 系数在处理类别不平衡的数据集时表现优异。它在较小的目标（如肿瘤）分割中，能够更好地反映模型的实际性能。

//#strong[缺点];：Dice 系数相对复杂，计算需要同时考虑预测结果与真实标签，尤其在处理多类别分割任务时，需要分别计算每个类别的 Dice 系数，再求平均值。

==== 二值交叉熵损失（BCE Loss）
<二值交叉熵损失bce-loss>

//二值交叉熵损失（Binary Cross-Entropy Loss）用于二分类任务，衡量模型预测值与真实值之间的差异。

//==== 公式
<公式>
对于每个像素的二值交叉熵损失定义如下：
$ upright("BCE") (p , g) = - (g log (p) + (1 - g) log (1 - p)) $
//其中，$p$ 是预测值，$g$ 是真实值。

整个图像的 BCE Loss 同样需要通过对所有像素的 BCE 损失求平均得到：
$ upright("BCE Loss") = 1 / N sum_(i = 1)^N upright("BCE") (p_i , g_i) $
//其中，$N$ 是图像中的总像素数。

BCE Loss 适用于二分类问题，常用于分割任务中的每个像素分类（如前景和背景的分类）。

//==== 验证与评估
//<验证与评估>
//交叉验证: 使用K折交叉验证（如5折）来更稳健地评估模型性能，避免过拟合。
//
//性能指标: 主要评估指标包括 Dice 相似系数（Dice Score）、Jaccard
//Index、Hausdorff距离、敏感性（Sensitivity）、特异性（Specificity）等。
//
//=== 模型优化与后处理
<模型优化与后处理>
////模型融合: 如果资源允许，则可以训练多个模型，并采用模型融合策略提高最终分割的准确性。

后处理算法:
应用形态学操作（如开运算、闭运算）平滑分割边界，去除小孤立区域，或使用条件随机场（CRF）后处理提高边界界定质量。

//== 实验结果
<实验结果>
//在搭建好的 SAM 大模型上实施肝脏肿瘤的分割任务，使用常见性能指标，准确度（Accuracy）、Dice 系数评价模型性能。通过这些指标，量化分析模型的分割效果，评估模型的训练效果并与其他分割方案比较。


= 结果分析与性能评估
<结果分析与性能比较>
为了全面评估基于 SAM 大模型的肝脏肿瘤分割软件的性能，选取主流的分割模型作为比较对象，U-Net 等在医学图像分割领域内已有大量成功应用，与之比较，可以更好的评估模型能力。

=== 交并比（IOU）
<交并比iou>
//==== 定义
<定义-1>
交并比（Intersection Over Union, IOU）也称为 Jaccard
Index，是一种常用的评估指标，用于衡量预测分割与真实分割之间的重叠程度。用于衡量预测分割结果与真实标注之间的一致性。IOU的计算公式表示如下：

$ upright("IoU") (A , B) = lr(|A sect B|) / lr(|A union B|) = frac(sum_i p_i g_i, sum_i p_i + sum_i g_i - sum_i p_i g_i) $

//其中，$p$ 是预测值，$g$ 是真实值。

//IOU 通常用作评估指标，而不是损失函数。它用于衡量分割结果与真实标签的重叠程度，广泛应用于各种分割任务的性能评估。
//在实际应用中，IoU通常用于： 
//1. #strong[评估分割质量];：通过计算预测的分割结果与真实标注的IoU，可以定量地评估分割算法的性能。
//2. #strong[确定阈值];：在某些任务中，可以通过设定一个IoU阈值来确定预测结果是否被认为是正确的。例如，如果IoU大于0.5，则认为分割是成功的。
//3. #strong[比较不同算法];：IOU可以用来比较不同图像分割算法的性能，帮助选择最优的算法。

//交并比是一个直观且广泛接受的指标，它能够很好地反映分割结果的准确性和完整性。


== 评估模型性能
<评估模型性能>
//通过微调训练，SAM 模型能够正确对腹部 CT 扫描图像中的肝脏肿瘤进行分割。//如图所示，肉眼几乎不可辨别分割真值与 SAM 模型分割结果之间的区别，而各其他分割算法均可看到分割结果与分割真值标签有肉眼可见的明显差异。

#figure(image("分割结果演示.png",width: 14cm),
  caption: [
    分割结果展示
  ]
)

#figure(image("其他模型分割结果.png"),
  caption: [
    其他模型的分割结果，从左至右依次为 原 CT
    图像、UNet、SegNet、DeepLabv3、FC-DenseNet、堆叠树形结构空洞卷积模型、分割真值标签
  ]
)

//以上介绍的评估指标中，DICE 系数与 DCE 结合作为模型训练中的损失函数使用，交并比（IOU）用于评估模型性能。交并比（IOU）反映了预测掩码与真实掩码之间的重叠程度，是一个介于0到1之间的值。IOU值越高，表示预测的分割结果与真实标注的一致性越好，通常用于评估分割算法的性能，尤其是在语义分割和实例分割任务中最为常用。

//在本研究中，如图 4.3 所示，通过对 SAM 模型的微调，模型对 CT 图像进行肝脏肿瘤分割的 IOU 指标可以达到 96% 以上，SAM 的预训练能力得到完美发挥，只需少量样本训练仍然能够表现出非常强大的分割能力。高质量的医学标注数据往往难以获得，因此，SAM 模型的强泛化能力对其他医学领域图像分割同样具有重要意义。

#figure(image("IOU折线图.png"),
  caption: [
    图 4.3 模型训练过程 IOU 结果，横坐标为训练轮次，纵坐标为验证集 IOU
    指标
  ]
)

//表 4.1 展示了不同模型在测试集上的分割性能比较，由 Ronneberger 等使用U-Net 模型给出的分割方法中，IOU值为0.920，Gao Fei等提出的基于堆叠树形聚合结构空洞卷积的肝脏肿瘤分割方法中，IOU 值为0.732，Zhou等使用 U-Net++ 模型提出的肝脏肿瘤分割方法中，IOU值为0.829，以上几种分割方法中，本文提到的基于 SAM的分割方法具有最好的 IOU 指标。

#figure(image("不同模型的肝脏肿瘤分割IOU（交并比）.png"),
  caption: [
    不同模型的肝脏肿瘤分割IOU
  ]
)

//上述结果中可以看出，本研究提出的基于 SAM 的分割方法在主要评估分割结果与真实值相似度的 IOU 指标上优于多种深度学习方法，基于 Transformer Vision 的SAM 模型在医学图像处理中表现优异。但本文研究方法和实验设计仍有提升空间，未来可通过更深入的模型调整和微调训练来改更进一步改善其性能，以提升模型在实际应用中的表现。

//== 结果讨论
<结果讨论>
//在本研究中，我们通过对万物分割模型（Segment Anything Model）在数据集上的微调，实现了对腹部 CT 扫描图像的肝脏肿瘤的自动分割。以下是实验结果的深入分析及其在实际应用中的改进方向。

//实验结果显示，基于 Vision Transformer 的 SAM 模型在腹部 CT 扫描图像的数据集上取得了较高准确率，这样的实验结果充分证实了 SAM 模型在肝脏肿瘤分割模型任务中的有效性。

////与以往的深度学习模型相比，本研究采用的 SAM 模型表现出更高的准确性能和更强的泛化能力。之前对于医学图像分割的研究通常依赖大量的数据集训练，例如 Gao Fei 等提出的基于堆叠树形聚合结构空洞卷积的肝脏肿瘤分割方法中，模型的训练使用了 2012 年至 2017 年在河南省人民医院接受肝癌治疗的 325 例晚期肝细胞肝癌（HCC）患者的CT图像，共获取 6146 张图像，其中 6046 张作为训练集，100 张作为测试集，经过数据增强和扩充后，训练集数据达到了20倍，即20×6046个图像。相比之下，本文提出的 SAM 分割方法训练所使用的数据集仅 20 例匿名患者的 CT 图像，训练成果却取得了远优于前者的分割效果。这进一步证实了 SAM 模型的优越性。

//SAM 作为新生的图像分割模型，诞生之初就被寄予厚望，SAM 模型的提出者 Jim Fan 认为，这是计算机视觉领域的 GPT-3 时刻。GPT-3 是一种强大的自然语言处理模型，通过预训练和微调，能够适应各种语言任务。同样地，SAM模型通过预训练和微调，能够适应各种图像分割任务。这意味着，SAM 有可能成为新的计算机视觉领域的发展方向，如 GPT 在语言文字处理领域的引领作用。通过这些讨论分析，我们有信心让 SAM 模型在医学图像分割领域表现出更高的性能和更强的适应性，更好地满足临床应用的需求。

//= 结论
<结论-2>
//本研究通过开发基于SAM大模型的肝脏肿瘤分割软件，展示了深度学习在医学图像分割领域的强大潜力。SAM大模型的引入，提高了分割精确度，尤其在处理腹部CT图像中肝脏肿瘤的复杂情况时表现出显著的优势。通过与其他流行模型的性能对比，本研究不仅证明了SAM大模型在肝脏肿瘤分割任务上的有效性，也为未来在此类应用中深度学习模型的优化提供了有价值的参考。

//进一步的分析和实验结果表明，虽然基于 SAM 大模型的分割软件在准确度上取得了优异的成绩，但仍存在计算效率和模型泛化能力方面的挑战。未来的工作可以着重于这些方面，探索更高效的算法或技术来提升模型性能，从而更好地服务于临床诊断和治疗规划。

//尽管 SAM 在肝脏肿瘤分割上显示出前景，但仍需针对医学图像的特点进行进一步优化和调整。未来的研究可以探索如何结合医学专家的知识和SAM的自动学习能力，以提高分割精度，减少需要手动调整的工作。同时，开发面向特定如肝脏肿瘤的深度学习模型，将是推动医学图像处理技术发展的关键。

//综上所述，基于 SAM 模型的肝脏肿瘤分割软件的开发，不仅可以改善现有的图像分割方法，还有助于提高肝脏疾病的诊断效率和准确性。未来的研究应当着重于模型的实际应用和临床转化，以实现医学影像自动化分析的最终目标。

//== 不足
<不足>
//+ #strong[数据集局限性];：
//  本研究所使用的数据集可能存在样本量不足或样本分布不均的问题，这可能导致模型在某些特定场景下的泛化能力受限。未来的研究可以通过引入更多样本量、更丰富的数据集，以及涵盖更多不同类型的肝脏肿瘤图像来解决这一问题。

//+ #strong[模型复杂性和计算资源需求];： 由于 SAM
  模型及其微调过程中涉及复杂的神经网络结构和大量的计算资源，训练和推理的时间和成本较高。在训练过程中冻结了模型的大部分内容，这在实际应用中可能限制其能力发挥。未来可以尝试通过模型压缩、剪枝和量化等技术来减少计算资源需求。

//+ #strong[实际应用验证不足];：
//  虽然本研究在测试集上展示了良好的性能，但在实际临床应用中的验证和评估尚不足。未来需要在真实临床环境中进行更多的测试和验证，以确保模型在实际应用中的可靠性和有效性。

//== 展望
//<展望>
//+ #strong[多模态数据融合];：
//  未来的研究可以探索将不同模态的医学图像（如MRI、CT、超声图像）进行融合，以提升模型的鲁棒性和准确性。多模态数据的融合可以提供更多的病灶信息，有助于更准确的肝脏肿瘤分割。

//+ #strong[模型的迁移学习与领域自适应];：
//  可以尝试将本研究的方法迁移到其他类型的医学图像分割任务中，如肺部、脑部等其他器官的肿瘤分割。同时，引入领域自适应技术，使模型能够在不同医院或不同设备生成的图像数据上保持高性能。

//+ #strong[实时分割与嵌入式应用];：
//  未来可以探索如何将微调后的SAM模型部署到嵌入式设备或实时系统中，以便在临床实践中实现实时的肝脏肿瘤分割。这需要在模型压缩和加速方面进行深入研究。

//+ #strong[集成学习与模型融合];：
//  可以尝试将SAM模型与其他先进的图像分割模型进行集成，利用集成学习的优势，进一步提升分割效果。模型融合技术可以综合多种模型的优点，提高最终分割结果的准确性和稳定性。

//+ #strong[临床合作与反馈];：
//  加强与临床专家的合作，通过他们的反馈不断改进和优化模型。这不仅有助于提升模型的实际应用价值，还能确保研究方向和临床需求紧密结合。

= 总结
<总结-1>
尽管本研究在利用 SAM 模型进行肝脏肿瘤分割方面取得了显著成果，但仍存在一些不足和挑战。未来的研究可以通过多模态数据融合、迁移学习、实时分割、集成学习和临床合作等多方面的努力，进一步提升模型性能，扩大其应用范围，最终实现更高效、更准确的医学图像分割系统。

在基于 SAM 大模型的肝脏肿瘤分割软件开发上取得了很好的成效，将来可通过对模型的进一步优化微调与前端开发提升其性能与易用性。我希望在以后的学习中能在该研究方面继续不懈学习和研究，也希望能通过自己的努力能够为医疗领域图像处理奉献力量。


